/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 * <p>
 * http://www.apache.org/licenses/LICENSE-2.0
 * <p>
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.mininglamp.common.tool;

import com.mininglamp.graph.exception.ParseError;
import com.mininglamp.graph.exception.ParseException;
import org.antlr.runtime.*;
import org.antlr.runtime.tree.CommonTree;
import org.antlr.runtime.tree.CommonTreeAdaptor;
import org.antlr.runtime.tree.TreeAdaptor;
import org.apache.hadoop.hive.ql.lib.Node;
import org.apache.hadoop.hive.ql.parse.*;

import java.util.ArrayList;


/**
 * ParseDriver.
 */
public class ParseDriver {
    String type = "test";

    /**
     * ANTLRNoCaseStringStream.
     * This class provides and implementation for a case insensitive token checker
     * for the lexical analysis part of antlr. By converting the token stream into
     * upper case at the time when lexical rules are checked, this class ensures that the
     * lexical rules need to just match the token with upper case letters as opposed to
     * combination of upper case and lower case characteres. This is purely used for matching lexical
     * rules. The actual token text is stored in the same way as the user input without
     * actually converting it into an upper case. The token values are generated by the consume()
     * function of the super class ANTLRStringStream. The LA() function is the lookahead funtion
     * and is purely used for matching lexical rules. This also means that the grammar will only
     * accept capitalized tokens in case it is run from other tools like antlrworks which
     * do not have the ANTLRNoCaseStringStream implementation.
     */

    public class ANTLRNoCaseStringStream extends ANTLRStringStream {

        public ANTLRNoCaseStringStream(String input) {
            super(input);
        }

        @Override
        public int LA(int i) {

            int returnChar = super.LA(i);
            if (returnChar == CharStream.EOF) {
                return returnChar;
            } else if (returnChar == 0) {
                return returnChar;
            }

            return Character.toUpperCase((char) returnChar);
        }
    }

    /**
     * HiveLexerX.
     */
    public class HiveLexerX extends HiveLexer {

        private final ArrayList<ParseError> errors;

        public HiveLexerX() {
            super();
            errors = new ArrayList<ParseError>();
        }

        public HiveLexerX(CharStream input) {
            super(input);
            errors = new ArrayList<ParseError>();
        }

        @Override
        public void displayRecognitionError(String[] tokenNames,
                                            RecognitionException e) {
            errors.add(new ParseError(this, e, tokenNames));
        }

        @Override
        public String getErrorMessage(RecognitionException e, String[] tokenNames) {
            String msg = null;

            if (e instanceof NoViableAltException) {
                @SuppressWarnings("unused")
                NoViableAltException nvae = (NoViableAltException) e;
                // for development, can add
                // "decision=<<"+nvae.grammarDecisionDescription+">>"
                // and "(decision="+nvae.decisionNumber+") and
                // "state "+nvae.stateNumber
                msg = "character " + getCharErrorDisplay(e.c) + " not supported here";
            } else {
                msg = super.getErrorMessage(e, tokenNames);
            }

            return msg;
        }

        public ArrayList<ParseError> getErrors() {
            return errors;
        }

    }

    /**
     * Tree adaptor for making antlr return ASTNodes instead of CommonTree nodes
     * so that the graph walking algorithms and the rules framework defined in
     * ql.lib can be used with the AST Nodes.
     */
    public static final TreeAdaptor adaptor = new CommonTreeAdaptor() {
        /**
         * Creates an ASTNode for the given token. The ASTNode is a wrapper around
         * antlr's CommonTree class that implements the Node interface.
         *
         * @param payload
         *          The token.
         * @return Object (which is actually an ASTNode) for the token.
         */
        @Override
        public Object create(Token payload) {
            return new ASTNode(payload);
        }

        @Override
        public Object dupNode(Object t) {

            return create(((CommonTree) t).token);
        }

        ;

        @Override
        public Object errorNode(TokenStream input, Token start, Token stop, RecognitionException e) {
            return new ASTErrorNode(input, start, stop, e);
        }

        ;
    };


    public ASTNode parse(String command)
            throws ParseException {
        return parse(command, true);
    }

    /**
     * Parses a command, optionally assigning the parser's token stream to the
     * given context.
     *
     * @param command command to parse
     * @return parsed AST
     * @author ctx     context with which to associate this parser's token stream, or
     * null if either no context is available or the context already has
     * an existing stream
     */
    public ASTNode parse(String command, boolean setTokenRewriteStream)
            throws ParseException {
//        System.out.println("172 ParseDriver Parsing command: " + command);

        HiveLexerX lexer = new HiveLexerX(new ANTLRNoCaseStringStream(command));
        TokenRewriteStream tokens = new TokenRewriteStream(lexer);

        HiveParser parser = new HiveParser(tokens);

        parser.setTreeAdaptor(adaptor);
        HiveParser.statement_return r;
//        if(command.indexOf("create table cpv.tb_ml_frequent_cus_month_avg_tmp")!=-1){
//            System.out.println(command);
//        }
        try {
            r = parser.statement();
        } catch (RecognitionException e) {
            e.printStackTrace();
            throw new RuntimeException("error");
            //throw new ParseException(parser.errors);
        }
        System.out.println("词法错误: " + lexer.getErrors().size());
        //System.out.println("语法错误: " + parser.errors.size());
        if (lexer.getErrors().size() == 0 //&& parser.errors.size() == 0
                ) {
            //System.out.println("5. ParseDriver 210 Parse Completed");
        } else if (lexer.getErrors().size() != 0) {
            throw new ParseException(lexer.getErrors());
        } else {
            //throw new ParseException(parser.errors);
            throw new RuntimeException("error");
        }

        ASTNode tree = (ASTNode) r.getTree();
        //System.out.println(tree.dump());
        tree.setUnknownTokenBoundaries();
        return tree;
    }


    /*
     * parse a String as a Select List. This allows table functions to be passed expression Strings
     * that are translated in
     * the context they define at invocation time. Currently used by NPath to allow users to specify
     * what output they want.
     * NPath allows expressions n 'tpath' a column that represents the matched set of rows. This
     * column doesn't exist in
     * the input schema and hence the Result Expression cannot be analyzed by the regular Hive
     * translation process.
     */
    public ASTNode parseSelect(String command) throws ParseException {
        System.out.println("Parsing command: " + command);

        HiveLexerX lexer = new HiveLexerX(new ANTLRNoCaseStringStream(command));
        TokenRewriteStream tokens = new TokenRewriteStream(lexer);

        HiveParser parser = new HiveParser(tokens);
        parser.setTreeAdaptor(adaptor);
        HiveParser_SelectClauseParser.selectClause_return r = null;
        try {
            r = parser.selectClause();
        } catch (RecognitionException e) {
            e.printStackTrace();
            //throw new ParseException(parser.errors);
            throw new RuntimeException("error");
        }

        if (lexer.getErrors().size() == 0 //&& parser.errors.size() == 0
                ) {
            //System.out.println("Parse complete!");
        } else if (lexer.getErrors().size() != 0) {
            throw new ParseException(lexer.getErrors());
        } else {
            //throw new ParseException(parser.errors);
            throw new RuntimeException("error");
        }

        return (ASTNode) r.getTree();
    }


    /*** 遍历sql 树
     *
     * @param lin
     * @param tree
     */
    public void travelTree(LineageNode lin, ASTNode tree, String sqlPath) {
        ArrayList<Node> children = tree.getChildren();
        if (children == null) {
            return;
        }
        for (Node node : children) {
            if (node instanceof ASTNode) {
                ASTNode ast = (ASTNode) node;

                if (TreeGlass.TOK_CREATETABLE.equals(ast.getText())) {
                    LNode n = new LNode();
                    n.setType(TreeGlass.TOK_CREATETABLE);
                    ArrayList<Node> ctbchilds = ast.getChildren();
                    for (Node ctbchild : ctbchilds) {
                        ASTNode ccd = (ASTNode) ctbchild;
                        if (TreeGlass.TOK_TABNAME.equals(ccd.getText())) {
                            ArrayList<Node> ctbtbs = ccd.getChildren();

                            StringBuffer sbuffer = new StringBuffer();
                            for (Node tabname : ctbtbs) {
                                sbuffer.append(tabname.toString() + ".");
                            }
                            n.setName(sbuffer.deleteCharAt(sbuffer.length() - 1).toString());
                        }
                    }
                    lin.setParent(n);
                    lin.setFilePath(sqlPath);
                } else if (TreeGlass.TOK_INSERT_INTO.equals(ast.getText())) {
                    LNode n = new LNode();
                    n.setType(TreeGlass.TOK_INSERT_INTO);
                    ArrayList<Node> ctbchilds = ast.getChildren();
                    for (Node ctbchild : ctbchilds) {
                        ASTNode ccd = (ASTNode) ctbchild;
                        if (TreeGlass.TOK_TAB.equals(ccd.getText())) {
                            ArrayList<Node> ctbtbs = ccd.getChildren();
                            for (Node ctbtb : ctbtbs) {
                                ASTNode astctb = (ASTNode) ctbtb;

                                if (TreeGlass.TOK_TABNAME.equals(astctb.getText())) {
                                    ArrayList<Node> ctbttbs = astctb.getChildren();

                                    StringBuffer sbuffer = new StringBuffer();
                                    for (Node tabname : ctbttbs) {
                                        sbuffer.append(tabname.toString() + ".");
                                    }
                                    n.setName(sbuffer.deleteCharAt(sbuffer.length() - 1).toString());
                                }
                            }

                        }
                    }
                    lin.setParent(n);
                    lin.setFilePath(sqlPath);
//                } else if (TreeGlass.TOK_TABREF.equals(ast.getText()) || TreeGlass.TOK_LIKETABLE.equals(ast.getText())) {
                } else if (true) {
                    if (TreeGlass.TOK_UNIONALL.equals(ast.getText())
                            || TreeGlass.TOK_LEFTOUTERJOIN.equals(ast.getText())
                            || TreeGlass.TOK_RIGHTOUTERJOIN.equals(ast.getText())
                            || TreeGlass.TOK_LIKETABLE.equals(ast.getText())
//                            || TreeGlass.TOK_TABREF.equals(ast.getText())
                            ) {
                        type = ast.getText();
                    }
                    if (TreeGlass.TOK_TABREF.equals(ast.getText()) || TreeGlass.TOK_LIKETABLE.equals(ast.getText())) {
                        LNode n = new LNode();
//                        n.setType(TreeGlass.TOK_TABREF);
                        n.setType(type);
                        ArrayList<Node> ctbchilds = ast.getChildren();
                        if (ctbchilds != null) {
                            for (Node ctbchild : ctbchilds) {
                                ASTNode ccd = (ASTNode) ctbchild;
                                if (ccd != null && TreeGlass.TOK_TABNAME.equals(ccd.getText())) {
                                    ArrayList<Node> ctbtbs = ccd.getChildren();
                                    StringBuffer sbuffer = new StringBuffer();
                                    for (Node tabname : ctbtbs) {
                                        sbuffer.append(tabname.toString() + ".");
                                    }
                                    n.setName(sbuffer.deleteCharAt(sbuffer.length() - 1).toString());
                                    n.setType(type);
                                    n.setFilePath(sqlPath);
                                }
                            }
                            lin.addChild(n);
                        }
                    }
                }
                travelTree(lin, (ASTNode) node, sqlPath);
            }
        }
    }


    public static void main(String[] args) {
        // String command = "select a.col1,a.col2 from taba a join " +
        // 		"(select b.col3 from tabb b) c on a.col1 = c.col3";
        String command = "create table staff.tb_ml_1 as select a.col1,a.col2 from staff.tb_ml_2 a join " +
                "(select col3 from staff.tb_ml_3) c on a.col1 = c.col3";
        String command2 = "create table staff.tb_ml_4 as " +
                "select * from staff.tb_ml_5 c join " +
                "staff.tb_ml_6 d on c.id = d.id where c.age > 25";
        //String command = "create table staff.tab1 like staff.tab2";
        //String command = "insert into staff.tab3 select c.col1,c.col2 from staff.tb4 c";
        ParseDriver pd = new ParseDriver();
        try {
            ASTNode tree = pd.parse(command);
            System.out.println(tree.dump());
            /***
             LineageNode lin = new LineageNode();
             pd.travelTree(lin,tree);
             if(lin.getParent() != null)
             System.out.println(lin.getParent().getName());
             for(LNode lnode:lin.getTabs()){
             System.out.println("|--"+lnode.getName());
             }
             ***/
        } catch (ParseException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }

    }
}
